{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于卷积神经网络的图像分类\n",
    "该notebook共训练了五个 神经网络，分别为 **Densenet \\ inceptionResnet v3 \\ Inception  V3 \\ Xception\\ Resnet50**     \n",
    "采用kaggle kernel +GPU的方式进行训练  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['speech-train2', 'speech-test']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.preprocessing import image \n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential,Model,load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,GlobalAveragePooling2D,GlobalMaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.optimizers import RMSprop,SGD\n",
    "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau, LearningRateScheduler\n",
    "import keras.backend as K\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import __version__\n",
    "from keras.applications.densenet import DenseNet201,preprocess_input\n",
    "from keras.applications.xception import Xception,preprocess_input\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) 训练集读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>accent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../input/speech-train2/train/train/10000.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../input/speech-train2/train/train/10001.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../input/speech-train2/train/train/10002.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../input/speech-train2/train/train/10003.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../input/speech-train2/train/train/10004.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        file_id accent\n",
       "0  ../input/speech-train2/train/train/10000.png      1\n",
       "1  ../input/speech-train2/train/train/10001.png      1\n",
       "2  ../input/speech-train2/train/train/10002.png      0\n",
       "3  ../input/speech-train2/train/train/10003.png      2\n",
       "4  ../input/speech-train2/train/train/10004.png      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将图片地址存入 dataframe ， 以flow_from_dataframe 方式读取图片数据\n",
    "train_df = pd.read_csv('../input/speech-train2/index.csv')\n",
    "train_df['accent'] = train_df['accent'].astype('str')\n",
    "print(type(train_df['accent'].values))\n",
    "train_df['file_id'] = \"../input/speech-train2/train/train/\" +train_df['file_id']\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras_preprocessing/image/image_data_generator.py:336: UserWarning: This ImageDataGenerator specifies `zca_whitening`, which overrides setting of `featurewise_center`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3825 validated image filenames belonging to 3 classes.\n",
      "Found 675 validated image filenames belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "IM_WIDTH, IM_HEIGHT = 128, 173 #densenet指定的图片尺寸\n",
    "\n",
    "nb_classes = 3 # 分类数\n",
    "batch_size = int(32)        \n",
    "target_size = (128,173)\n",
    "train_shape =  4500*0.85 # 训练样本个数3600\n",
    "val_shape = 4500*0.15  # 验证样本个数\n",
    "\n",
    "\n",
    "# 训练数据与测试数据\n",
    "\n",
    "datagen = image.ImageDataGenerator(preprocessing_function=None,   #preprocess_input\n",
    "                             rescale=1./255.,\n",
    "                            zca_whitening = True,\n",
    "#                               width_shift_range=0.1, \n",
    "#                                height_shift_range=0.1, \n",
    "                             validation_split=0.15)  #0.2\n",
    " \n",
    "train_generator=datagen.flow_from_dataframe(\n",
    "                        dataframe=train_df,\n",
    "                        directory=None, \n",
    "                        x_col=\"file_id\", \n",
    "                        y_col=\"accent\",\n",
    "                        has_ext=False,\n",
    "                        subset=\"training\",\n",
    "                        batch_size=batch_size,\n",
    "                        seed=42,\n",
    "                        shuffle=True,\n",
    "                        class_mode=\"categorical\",\n",
    "                        target_size=target_size)  #(128, 173)\n",
    "\n",
    "\n",
    "valid_generator=datagen.flow_from_dataframe(\n",
    "                        dataframe=train_df,\n",
    "                        directory=None,\n",
    "                        x_col=\"file_id\",\n",
    "                        y_col=\"accent\",\n",
    "                        has_ext=False,\n",
    "                        subset=\"validation\",\n",
    "                        batch_size=batch_size, #1\n",
    "                        seed=42,\n",
    "                        shuffle=True,\n",
    "                        class_mode=\"categorical\",\n",
    "                        target_size=target_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) 测试集读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5377, 2)\n",
      "Found 5377 validated image filenames.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>accent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../input/speech-test/test/test/21352.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../input/speech-test/test/test/21121.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../input/speech-test/test/test/23559.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../input/speech-test/test/test/20365.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../input/speech-test/test/test/24472.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    file_id accent\n",
       "0  ../input/speech-test/test/test/21352.png    NaN\n",
       "1  ../input/speech-test/test/test/21121.png    NaN\n",
       "2  ../input/speech-test/test/test/23559.png    NaN\n",
       "3  ../input/speech-test/test/test/20365.png    NaN\n",
       "4  ../input/speech-test/test/test/24472.png    NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试集\n",
    "\n",
    "folder_dir = \"../input/speech-test/test/test/\"  #测试集\n",
    "\n",
    "test_df = pd.DataFrame(columns=['file_id','accent'])\n",
    "i= 0 \n",
    "\n",
    "for filename in os.listdir(folder_dir):\n",
    "    file = folder_dir+filename\n",
    "    test_df.at[i,'file_id'] = file\n",
    "    i+=1\n",
    "\n",
    "print(test_df.shape)\n",
    "\n",
    "test_datagen=image.ImageDataGenerator(rescale=1./255.,\n",
    "                                     preprocessing_function=None,   #preprocess_input\n",
    "                             )\n",
    "\n",
    "test_generator=test_datagen.flow_from_dataframe(\n",
    "                            dataframe=test_df,\n",
    "                            directory=None,\n",
    "                            x_col=\"file_id\",\n",
    "                            y_col=None,\n",
    "                            has_ext=False,\n",
    "                            batch_size=batch_size,#1\n",
    "                            seed=42,\n",
    "                            shuffle=False,\n",
    "                            class_mode=None,\n",
    "                            target_size=target_size)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （3）inception v3测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.xception import Xception,preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(base_model,output_model_file):\n",
    "    nb_classes = 3 # 分类数\n",
    "    nb_epoch = int(30)                # epoch数量\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predicts =Dense(3, activation='softmax')(x) # for i in range(3) ] \n",
    "\n",
    "\n",
    "    v3_model = Model(inputs=base_model.input, outputs=predicts)  #\n",
    "    v3_model.compile(\n",
    "                    optimizer='adam',\n",
    "\n",
    "                    loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    v3_output_model_file =output_model_file #'v3_checkpoint-{epoch:02d}e-val_acc_{val_acc:.2f}.hdf5'\n",
    "\n",
    "    v3_checkpoint = ModelCheckpoint(v3_output_model_file, monitor='val_acc',\n",
    "                                 verbose=1, save_best_only=True)\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\n",
    "                                                    patience=4,\n",
    "                                                    verbose=2,\n",
    "                                                    factor=.3,\n",
    "                                                    min_lr=.00001)\n",
    "\n",
    "    v3_history_ft = v3_model.fit_generator(\n",
    "                                    train_generator,\n",
    "                                    nb_epoch=nb_epoch,  #\n",
    "                                    validation_data=valid_generator,\n",
    "                                    validation_steps=val_shape // batch_size,\n",
    "\n",
    "                                    steps_per_epoch=train_shape // batch_size,\n",
    "                                    callbacks=[learning_rate_reduction,v3_checkpoint], #\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "219062272/219055592 [==============================] - 8s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=<keras_pre..., validation_steps=21.0, steps_per_epoch=119.0, callbacks=[<keras.ca..., epochs=30)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/opt/conda/lib/python3.6/site-packages/keras_preprocessing/image/image_data_generator.py:735: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 [==============================] - 90s 754ms/step - loss: 1.0073 - acc: 0.5454 - val_loss: 1.9908 - val_acc: 0.4851\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.48512, saving model to dense_checkpoint-01e-val_acc_0.49.hdf5\n",
      "Epoch 2/30\n",
      "119/119 [==============================] - 41s 347ms/step - loss: 0.7709 - acc: 0.6809 - val_loss: 1.7794 - val_acc: 0.3624\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.48512\n",
      "Epoch 3/30\n",
      "119/119 [==============================] - 41s 341ms/step - loss: 0.6440 - acc: 0.7453 - val_loss: 1.6223 - val_acc: 0.5863\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.48512 to 0.58631, saving model to dense_checkpoint-03e-val_acc_0.59.hdf5\n",
      "Epoch 4/30\n",
      "119/119 [==============================] - 41s 345ms/step - loss: 0.5442 - acc: 0.7978 - val_loss: 4.9937 - val_acc: 0.4603\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.58631\n",
      "Epoch 5/30\n",
      "119/119 [==============================] - 40s 339ms/step - loss: 0.4714 - acc: 0.8283 - val_loss: 8.2243 - val_acc: 0.3561\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.58631\n",
      "Epoch 6/30\n",
      "119/119 [==============================] - 40s 340ms/step - loss: 0.2491 - acc: 0.9153 - val_loss: 0.9259 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.58631 to 0.71695, saving model to dense_checkpoint-06e-val_acc_0.72.hdf5\n",
      "Epoch 7/30\n",
      "119/119 [==============================] - 41s 344ms/step - loss: 0.1784 - acc: 0.9368 - val_loss: 1.2716 - val_acc: 0.6501\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.71695\n",
      "Epoch 8/30\n",
      "119/119 [==============================] - 40s 340ms/step - loss: 0.1424 - acc: 0.9509 - val_loss: 1.7096 - val_acc: 0.6314\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.71695\n",
      "Epoch 9/30\n",
      "119/119 [==============================] - 41s 346ms/step - loss: 0.1056 - acc: 0.9662 - val_loss: 1.0074 - val_acc: 0.7154\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.71695\n",
      "Epoch 10/30\n",
      "119/119 [==============================] - 40s 340ms/step - loss: 0.0846 - acc: 0.9703 - val_loss: 1.3758 - val_acc: 0.6423\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.71695\n",
      "Epoch 11/30\n",
      "119/119 [==============================] - 40s 340ms/step - loss: 0.0294 - acc: 0.9913 - val_loss: 0.7717 - val_acc: 0.7807\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.71695 to 0.78072, saving model to dense_checkpoint-11e-val_acc_0.78.hdf5\n",
      "Epoch 12/30\n",
      "119/119 [==============================] - 41s 344ms/step - loss: 0.0083 - acc: 0.9979 - val_loss: 1.0798 - val_acc: 0.7465\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.78072\n",
      "Epoch 13/30\n",
      "119/119 [==============================] - 41s 340ms/step - loss: 0.0121 - acc: 0.9963 - val_loss: 1.0917 - val_acc: 0.7527\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.78072\n",
      "Epoch 14/30\n",
      "119/119 [==============================] - 41s 340ms/step - loss: 0.0071 - acc: 0.9982 - val_loss: 0.9068 - val_acc: 0.7994\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.78072 to 0.79938, saving model to dense_checkpoint-14e-val_acc_0.80.hdf5\n",
      "Epoch 15/30\n",
      "119/119 [==============================] - 41s 346ms/step - loss: 0.0087 - acc: 0.9979 - val_loss: 1.1598 - val_acc: 0.7776\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.79938\n",
      "Epoch 16/30\n",
      "119/119 [==============================] - 41s 345ms/step - loss: 0.0072 - acc: 0.9979 - val_loss: 1.4124 - val_acc: 0.7170\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.79938\n",
      "Epoch 17/30\n",
      "119/119 [==============================] - 40s 340ms/step - loss: 0.0056 - acc: 0.9987 - val_loss: 1.3505 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.79938\n",
      "Epoch 18/30\n",
      "119/119 [==============================] - 40s 339ms/step - loss: 0.0295 - acc: 0.9899 - val_loss: 1.0064 - val_acc: 0.7683\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.79938\n",
      "Epoch 19/30\n",
      "119/119 [==============================] - 41s 341ms/step - loss: 0.0115 - acc: 0.9969 - val_loss: 0.9501 - val_acc: 0.7838\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.79938\n",
      "Epoch 20/30\n",
      "119/119 [==============================] - 40s 340ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.9717 - val_acc: 0.7776\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.79938\n",
      "Epoch 21/30\n",
      "119/119 [==============================] - 40s 339ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 1.0517 - val_acc: 0.7729\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.79938\n",
      "Epoch 22/30\n",
      "119/119 [==============================] - 40s 340ms/step - loss: 0.0018 - acc: 0.9995 - val_loss: 1.0594 - val_acc: 0.7807\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.79938\n",
      "Epoch 23/30\n",
      "119/119 [==============================] - 41s 346ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 1.0852 - val_acc: 0.7798\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.79938\n",
      "Epoch 24/30\n",
      "119/119 [==============================] - 41s 341ms/step - loss: 8.9289e-04 - acc: 1.0000 - val_loss: 1.1029 - val_acc: 0.7729\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.79938\n",
      "Epoch 25/30\n",
      "119/119 [==============================] - 40s 339ms/step - loss: 7.7049e-04 - acc: 1.0000 - val_loss: 1.1149 - val_acc: 0.7776\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.79938\n",
      "Epoch 26/30\n",
      "119/119 [==============================] - 40s 340ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 1.0825 - val_acc: 0.7807\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.79938\n",
      "Epoch 27/30\n",
      "119/119 [==============================] - 40s 340ms/step - loss: 9.8895e-04 - acc: 0.9997 - val_loss: 1.1241 - val_acc: 0.7838\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.79938\n",
      "Epoch 28/30\n",
      "119/119 [==============================] - 41s 341ms/step - loss: 7.6224e-04 - acc: 1.0000 - val_loss: 1.1100 - val_acc: 0.7807\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.79938\n",
      "Epoch 29/30\n",
      "119/119 [==============================] - 40s 339ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 1.1974 - val_acc: 0.7714\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.79938\n",
      "Epoch 30/30\n",
      "119/119 [==============================] - 40s 338ms/step - loss: 9.3520e-04 - acc: 0.9997 - val_loss: 1.2614 - val_acc: 0.7621\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.79938\n"
     ]
    }
   ],
   "source": [
    "# 模型训练\n",
    "# dense_model = DenseNet201(include_top=False)\n",
    "# model_training( dense_model , output_model_file = 'dense_checkpoint-{epoch:02d}e-val_acc_{val_acc:.2f}.hdf5')\n",
    "\n",
    "# v3_model = InceptionV3(include_top=False ) \n",
    "# model_training( v3_model , output_model_file = 'v3_checkpoint-{epoch:02d}e-val_acc_{val_acc:.2f}.hdf5')\n",
    "\n",
    "# x_model = Xception(include_top=False )\n",
    "# model_training( x_model , output_model_file = 'v3_checkpoint-{epoch:02d}e-val_acc_{val_acc:.2f}.hdf5')\n",
    "\n",
    "v2_model = InceptionResNetV2(include_top=False )\n",
    "model_training( v2_model , output_model_file = 'dense_checkpoint-{epoch:02d}e-val_acc_{val_acc:.2f}.hdf5')\n",
    "\n",
    "# v2_model = ResNet50(include_top=False )\n",
    "# model_training( v2_model , output_model_file = 'dense_checkpoint-{epoch:02d}e-val_acc_{val_acc:.2f}.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 模型存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dense_checkpoint-03e-val_acc_0.59.hdf5', 'dense_checkpoint-06e-val_acc_0.72.hdf5', 'dense_checkpoint-11e-val_acc_0.78.hdf5', 'dense_checkpoint-01e-val_acc_0.49.hdf5', '__notebook__.ipynb', '__output__.json', 'dense_checkpoint-14e-val_acc_0.80.hdf5']\n",
      "\n",
      "[]\n",
      "['dense_checkpoint-01e-val_acc_0.49.hdf5', 'dense_checkpoint-03e-val_acc_0.59.hdf5', 'dense_checkpoint-06e-val_acc_0.72.hdf5', 'dense_checkpoint-11e-val_acc_0.78.hdf5', 'dense_checkpoint-14e-val_acc_0.80.hdf5']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files = os.listdir(\"../working\")\n",
    "print(files)\n",
    "print()\n",
    "v3_checkpoint = []\n",
    "dense_checkpoint = []\n",
    "for file in files:\n",
    "\n",
    "    if file.startswith('v3_checkpoint'):  # multi checkpoint\n",
    "        v3_checkpoint.append(file)\n",
    "    if file.startswith('dense_checkpoint'):  # multi checkpoint\n",
    "        dense_checkpoint.append(file)\n",
    "        \n",
    "v3_checkpoint = sorted(v3_checkpoint)      \n",
    "dense_checkpoint = sorted(dense_checkpoint)\n",
    "print((v3_checkpoint))\n",
    "print(dense_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(model , test_generator, name):\n",
    "    # 模型预测\n",
    "\n",
    "    pred = model.predict_generator(test_generator,steps= len(test_generator)) \n",
    "    \n",
    "    df = pd.DataFrame(columns=['file_id','accent'])  #输出文件\n",
    "    i= 0\n",
    "\n",
    "    predicted_class_indices = np.argmax(pred, axis=1)\n",
    "\n",
    "    \n",
    "    # # #建立预测结果和文件名之间的关系\n",
    "    filenames = test_generator.filenames\n",
    "    for idx in range(len(filenames )):\n",
    "        df.at[i,'file_id'] = filenames[idx]\n",
    "        df.at[i,'accent'] = (int(predicted_class_indices[idx]))\n",
    "        i += 1\n",
    "#         print('predict  %d' % (int(predicted_class_indices[idx])),'title    %s' % filenames[idx])\n",
    "\n",
    "\n",
    "    df['file_id'] = df['file_id'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "    df = df.sort_values(by = 'file_id' , ascending = True)\n",
    "    print(df.head())\n",
    "    df.to_csv(name,index = False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     file_id accent\n",
      "1753   20000      1\n",
      "2831   20001      1\n",
      "2603   20002      1\n",
      "1355   20003      0\n",
      "3850   20004      2\n",
      "0    2129\n",
      "1    1628\n",
      "2    1620\n",
      "Name: accent, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# v3 模型输出\n",
    "# v3_model = load_model(v3_checkpoint[-1])\n",
    "# df = output(v3_model ,test_generator,'x_submission.csv')  # 利用已加载的\n",
    "# print(df.accent.value_counts())\n",
    "\n",
    "#densenet 模型输出\n",
    "dense_model = load_model(dense_checkpoint[-1])\n",
    "df = output(dense_model ,test_generator,'v2_submission.csv')  # 利用已加载的\n",
    "print(df.accent.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) bottleneck vgg16提取特征  \n",
    "经试验验证，发现 此方法效果并不好，因此不采用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimg_width, img_height = 173, 128\\n\\nclass_indics = 'class_indices.npy'\\nbottleneck_train_path = 'bottleneck_features_train.npy'\\nbottleneck_validation_path = 'bottleneck_features_validation.npy'\\ntop_model_weights_path = 'bottleneck_fc_model.h5'\\ntrain_data_dir = 'data/train'\\nvalidation_data_dir = 'data/validation/'\\n\\nnb_train_samples = 3600\\nnb_validation_samples = 900\\n\\nepochs = 20\\nbatch_size = 32\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "img_width, img_height = 173, 128\n",
    "\n",
    "class_indics = 'class_indices.npy'\n",
    "bottleneck_train_path = 'bottleneck_features_train.npy'\n",
    "bottleneck_validation_path = 'bottleneck_features_validation.npy'\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation/'\n",
    "\n",
    "nb_train_samples = 3600\n",
    "nb_validation_samples = 900\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  获取generator 标签\n",
    "index = pd.read_csv('../input/speech-train2/index.csv')\n",
    "\n",
    "def get_labels(generator,index, ):\n",
    "    labels = []\n",
    "    for i in generator.filenames:\n",
    "        name = i.split('/')[-1]\n",
    "        label = index[index['file_id']==name].accent.values[0]\n",
    "#         print( name ,label)\n",
    "        labels.append(label)\n",
    "    \n",
    "    labels = keras.utils.to_categorical(labels,num_classes=3)\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_indics():\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    train_generator=datagen.flow_from_dataframe(\n",
    "                        dataframe=train_df,\n",
    "                        directory=None, \n",
    "                        x_col=\"file_id\", \n",
    "                        y_col=\"accent\",\n",
    "                        has_ext=False,\n",
    "                        batch_size=batch_size,\n",
    "                        seed=42,\n",
    "                        class_mode=\"categorical\",\n",
    "                        target_size=(128, 173))  #(128, 173)\n",
    "\n",
    "    # save the class indices to use later in predictions\n",
    "    np.save(class_indics, train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bottleneck_features(train_generator,valid_generator):\n",
    "    print('Using of bottleneck feature on pretrained model started.')\n",
    "    \n",
    "    # build the VGG16 network\n",
    "#     model = VGG16(include_top=False, weights='imagenet')\n",
    "    model = Xception(include_top=False)\n",
    "\n",
    "    \n",
    "    bottleneck_features_train = model.predict_generator( train_generator, \n",
    "                                                        steps = len(train_generator)  #nb_train_samples // batch_size\n",
    "                                                       )\n",
    "    \n",
    "    np.save(open(bottleneck_train_path, 'wb'), bottleneck_features_train)\n",
    "    print('train bottleneck shape',bottleneck_features_train.shape)\n",
    "\n",
    "    bottleneck_features_validation = model.predict_generator(valid_generator,\n",
    "                                                             steps = len(valid_generator)#nb_validation_samples // batch_size\n",
    "                                                            )\n",
    "    np.save(open(bottleneck_validation_path, 'wb'), bottleneck_features_validation)\n",
    "    print('validation bottleneck shape',bottleneck_features_validation.shape)\n",
    "    print('Using of bottleneck feature on pretrained model finished.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_top_model():\n",
    "    print('Training of top model started.')\n",
    "    \n",
    "    train_data = np.load(open(bottleneck_train_path, 'rb'))\n",
    "    validation_data = np.load(open(bottleneck_validation_path, 'rb'))\n",
    "\n",
    "\n",
    "    train_labels = get_labels(train_generator,index )\n",
    "    val_labels = get_labels(valid_generator,index)\n",
    "    \n",
    "    class_dictionary = np.load('class_indices.npy', allow_pickle=True).item()\n",
    "    num_classes = len(class_dictionary)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.7))\n",
    "    model.add(Dense(num_classes, activation='softmax')) #sigmoid\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])  #categorical_\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, val_labels))\n",
    "    \n",
    "    model.save_weights(top_model_weights_path)\n",
    "    print('Training of top model completed & saved as: ',top_model_weights_path)\n",
    "\n",
    "\n",
    "def fine_tune_pretrained_model():\n",
    "    print('Fine tuning of pretrain model started.')\n",
    "    # build the VGG16 network\n",
    "    input_tensor = Input(shape=(128, 173, 3))\n",
    "\n",
    "#     base_model = applications.VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "#     base_model = applications.VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "    base_model = Xception(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "\n",
    "\n",
    "    class_dictionary = np.load('class_indices.npy', allow_pickle=True).item()\n",
    "    num_classes = len(class_dictionary)\n",
    "\n",
    "    # build a classifier model to put on top of the convolutional model\n",
    "    top_model = Sequential()\n",
    "    top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "    top_model.add(Dense(1024, activation='relu'))\n",
    "    top_model.add(Dropout(0.3))\n",
    "    top_model.add(Dense(num_classes, activation='softmax')) #sigmoid\n",
    "\n",
    "    # note that it is necessary to start with a fully-trained\n",
    "    # classifier, including the top classifier,\n",
    "    # in order to successfully do fine-tuning\n",
    "    top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "    # add the model on top of the convolutional base\n",
    "    model = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n",
    "\n",
    "    # set the first 25 layers (up to the last conv block)\n",
    "    # to non-trainable (weights will not be updated)\n",
    "    for layer in model.layers[:25]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # compile the model with a SGD/momentum optimizer\n",
    "    # and a very slow learning rate.\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "#                   optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "                  optimizer = 'adam',\n",
    "                  metrics=['accuracy'])  #categorical_\n",
    "\n",
    "    # prepare data augmentation configuration\n",
    "#     train_datagen = ImageDataGenerator(\n",
    "#         rescale=1. / 255,\n",
    "        \n",
    "#         horizontal_flip=True)\n",
    "\n",
    "#     test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "#     train_generator = train_datagen.flow_from_directory(\n",
    "#         train_data_dir,\n",
    "#         target_size=(img_height, img_width),\n",
    "#         batch_size=batch_size,\n",
    "#         class_mode='categorical')\n",
    "\n",
    "#     validation_generator = test_datagen.flow_from_directory(\n",
    "#         validation_data_dir,\n",
    "#         target_size=(img_height, img_width),\n",
    "#         batch_size=batch_size,\n",
    "#         class_mode='categorical')\n",
    "\n",
    "#     # fine-tune the model\n",
    "#     model.fit_generator(\n",
    "#         train_generator,\n",
    "#         steps_per_epoch=nb_train_samples // batch_size, # samples_per_epoch=nb_train_samples,\n",
    "#         epochs=epochs,\n",
    "#         validation_data=validation_generator,\n",
    "#         validation_steps=nb_validation_samples)\n",
    "\n",
    "#     print('Fine tuning of pretrain model completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v2_submission.csv', 'dense_checkpoint-03e-val_acc_0.59.hdf5', 'dense_checkpoint-06e-val_acc_0.72.hdf5', 'dense_checkpoint-11e-val_acc_0.78.hdf5', 'dense_checkpoint-01e-val_acc_0.49.hdf5', '__notebook__.ipynb', '__output__.json', 'dense_checkpoint-14e-val_acc_0.80.hdf5']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('../working'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif not os.path.exists(class_indics):\\n    generate_class_indics()\\n\\nsave_bottleneck_features(train_generator , valid_generator)\\n\\ntrain_top_model()\\nfine_tune_pretrained_model()\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "if not os.path.exists(class_indics):\n",
    "    generate_class_indics()\n",
    "\n",
    "save_bottleneck_features(train_generator , valid_generator)\n",
    "\n",
    "train_top_model()\n",
    "fine_tune_pretrained_model()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
