{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "### 该文件主要用于功能测试实验\n",
    "进行了以下几个实验：     \n",
    "0）自定义CNN网络，自定义网络训练速度快，但是精度非常低，只有0.3左右      \n",
    "1）bottleneck特征提取，效果不好，最后没有采用        \n",
    "2）densenet xception模型并联融合，精度与单个模型精度相当，但是在模型存贮过程中会超出内存限制，因此也没有采用    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['speech-train2', 'model3-20epoch081', 'speech-test', 'model3-1cnn2', 'model3-10epoch058', 'multimodel3', 'multimodel2', 'model3-20epoch059', 'model3-30epoch0398', 'densemodel', 'model3', 'multimodel']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 173, 4)\n"
     ]
    }
   ],
   "source": [
    "# 查看图像信息\n",
    "import matplotlib.image as mpimg\n",
    "im = mpimg.imread('../input/speech-test/test/test/23330.png')\n",
    "print(im.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing import image \n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,AveragePooling2D,Convolution2D,Reshape\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.callbacks import ReduceLROnPlateau, LearningRateScheduler\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2-1.建立CNN自定义模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def CNN_model(train_generator, valid_generator, epochs=50, batch_size=128):  # 生成器generator格式\n",
    "def CNN_model(train_generator, valid_generator, epochs=50, batch_size=32):\n",
    "    # 模型框架构建\n",
    "    model = Sequential()\n",
    "\n",
    "#     model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='Same',\n",
    "#                      activation='relu', input_shape=(128, 173, 3)))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='Same',\n",
    "#                      activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(.25))\n",
    "    \n",
    "#     model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='Same',\n",
    "#                      activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='Same',\n",
    "#                      activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "#     model.add(Dropout(.25))\n",
    "    \n",
    "#     model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='Same',\n",
    "#                      activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(.25))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(256, activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(.25))\n",
    "#     model.add(Dense(3, activation='softmax'))   # 3个节点\n",
    "\n",
    "    model.add(BatchNormalization(input_shape=(64, 87, 3))) #\n",
    "    \n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='Same',\n",
    "                     activation='relu',))  #(128, 173, 3)\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(.2))\n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='Same',\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(.25))\n",
    "    \n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='Same',\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(.25))\n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='Same',\n",
    "                     activation='relu'))\n",
    "    model.add(Dropout(.3))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(3, activation='softmax'))   # 3个节点\n",
    "    \n",
    "\n",
    "    # 模型编译\n",
    "    model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])#\n",
    "\n",
    "    # 学习率的annealing模拟退火\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\n",
    "                                                patience=4,\n",
    "                                                verbose=2,\n",
    "                                                factor=.5,\n",
    "                                                min_lr=.00001)\n",
    "\n",
    "    # 图像随机旋转10度, 0.9-1.1随机缩放，水平和垂直方向随机便宜0.1幅度\n",
    "#     rotation_range = 10\n",
    "#     zoom_range = .1\n",
    "#     height_shift_range = .1\n",
    "#     width_shift_range = .1\n",
    "#     # 数据扩增\n",
    "#     data_gen = ImageDataGenerator(rotation_range=rotation_range,\n",
    "#                                   zoom_range=zoom_range,\n",
    "#                                   height_shift_range=height_shift_range,\n",
    "#                                   width_shift_range=width_shift_range)\n",
    "#     data_gen.fit(x_train)\n",
    "\n",
    "    # model.fit_generator会返回一个history对象，该对象记录了训练过程中的运行输出\n",
    "    # data_gen.flow是一个keras的生成器函数，会无限地进行循环，不会返回或退出\n",
    "    history = model.fit_generator(train_generator,\n",
    "                                  epochs=epochs,\n",
    "                                  validation_data=valid_generator,\n",
    "                                  validation_steps=val_shape // batch_size,\n",
    "                                  verbose=2,\n",
    "                                  steps_per_epoch=train_shape // batch_size,\n",
    "                                  callbacks=[learning_rate_reduction]\n",
    "                                 )\n",
    "\n",
    "    return history, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单次验证                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
    "# history, model = CNN_model(train_generator, valid_generator)\n",
    "# i = 0 \n",
    "# print('fold {0:d}: Epochs=50, Training accuracy={1:.5f}, Validation accuracy={2:.5f}'.format(i + 1,\n",
    "#                                                                             max(history.history['acc']),\n",
    "#                                                                             max(history.history['val_acc'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单次预测\n",
    "# pred = model.predict_generator(test_generator,steps= len(test_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df = pd.DataFrame(columns=['file_id','accent'])  #输出文件\n",
    "# i= 0\n",
    "\n",
    "# predicted_class_indices = np.argmax(pred, axis=1)\n",
    "# labels = (train_generator.class_indices)\n",
    "# label = dict((v,k) for k,v in labels.items())\n",
    "# print('labels',labels)\n",
    "# print('label',label)\n",
    "# # 建立代码标签与真实标签的关系\n",
    "# predictions = [label[i] for i in predicted_class_indices]\n",
    "\n",
    "# # #建立预测结果和文件名之间的关系\n",
    "# filenames = test_generator.filenames\n",
    "# for idx in range(len(filenames )):\n",
    "#     df.at[i,'file_id'] = filenames[idx]\n",
    "#     df.at[i,'accent'] = (int(predictions[idx]))\n",
    "#     i += 1\n",
    "# #     print('predict  %d' % (int(predictions[idx])))\n",
    "# #     print('title    %s' % filenames[idx])\n",
    "# df['file_id'] = df['file_id'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "# df = df.sort_values(by = 'file_id' , ascending = True)\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.accent.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出结果\n",
    "\n",
    "# df.to_csv('submission.csv',index = False)\n",
    "# print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. DenseNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2-0 模型概览"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.densenet import DenseNet201,preprocess_input\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2-1 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import __version__\n",
    "from keras.applications.densenet import DenseNet201,preprocess_input\n",
    " \n",
    "from keras.models import Model,load_model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing import image \n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>accent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../input/speech-train2/train/train/10000.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../input/speech-train2/train/train/10001.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../input/speech-train2/train/train/10002.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../input/speech-train2/train/train/10003.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../input/speech-train2/train/train/10004.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        file_id accent\n",
       "0  ../input/speech-train2/train/train/10000.png      1\n",
       "1  ../input/speech-train2/train/train/10001.png      1\n",
       "2  ../input/speech-train2/train/train/10002.png      0\n",
       "3  ../input/speech-train2/train/train/10003.png      2\n",
       "4  ../input/speech-train2/train/train/10004.png      1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../input/speech-train2/index.csv')\n",
    "train_df['accent'] = train_df['accent'].astype('str')\n",
    "print(type(train_df['accent'].values))\n",
    "train_df['file_id'] = \"../input/speech-train2/train/train/\" +train_df['file_id']\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras_preprocessing/image/image_data_generator.py:336: UserWarning: This ImageDataGenerator specifies `zca_whitening`, which overrides setting of `featurewise_center`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3600 validated image filenames belonging to 3 classes.\n",
      "Found 900 validated image filenames belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#  训练集、测试集\n",
    "# 数据准备\n",
    "IM_WIDTH, IM_HEIGHT = 128, 173 #densenet指定的图片尺寸\n",
    "\n",
    "nb_classes = 3 # 分类数\n",
    "batch_size = int(32)        \n",
    "target_size = (128,173)\n",
    "train_shape = 3600  # 训练样本个数\n",
    "val_shape = 900  # 验证样本个数\n",
    "\n",
    "\n",
    "# 训练数据与测试数据\n",
    "\n",
    "datagen = image.ImageDataGenerator(preprocessing_function=preprocess_input,   #None\n",
    "                             rescale=1./255.,\n",
    "                            zca_whitening = True,\n",
    "                             validation_split=0.2)\n",
    " \n",
    "train_generator=datagen.flow_from_dataframe(\n",
    "                        dataframe=train_df,\n",
    "                        directory=None, \n",
    "                        x_col=\"file_id\", \n",
    "                        y_col=\"accent\",\n",
    "                        has_ext=False,\n",
    "                        subset=\"training\",\n",
    "                        batch_size=batch_size,\n",
    "                        seed=42,\n",
    "                        shuffle=True,\n",
    "                        class_mode=\"categorical\",\n",
    "                        target_size=target_size)  #(128, 173)\n",
    "\n",
    "\n",
    "valid_generator=datagen.flow_from_dataframe(\n",
    "                        dataframe=train_df,\n",
    "                        directory=None,\n",
    "                        x_col=\"file_id\",\n",
    "                        y_col=\"accent\",\n",
    "                        has_ext=False,\n",
    "                        subset=\"validation\",\n",
    "                        batch_size=batch_size, #1\n",
    "                        seed=42,\n",
    "                        shuffle=True,\n",
    "                        class_mode=\"categorical\",\n",
    "                        target_size=target_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) DenseNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 添加新层\n",
    "def add_new_last_layer(base_model, nb_classes):\n",
    "\n",
    "   # 添加最后的层\n",
    "   # 输入\n",
    "   # base_model和分类数量\n",
    "   # 输出\n",
    "   # 新的keras的model\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    predictions = Dense(nb_classes, activation='softmax')(x) #new softmax layer\n",
    "    model = Model(input=base_model.input, output=predictions)\n",
    "    return model\n",
    " \n",
    "\n",
    "#搭建模型\n",
    "model = DenseNet201(include_top=False)  #  可尝试是否制定 预训练好的权重\n",
    "model = add_new_last_layer(model, nb_classes)\n",
    "#model.load_weights('../model/checkpoint-02e-val_acc_0.82.hdf5')  第二次训练可以接着第一次训练得到的模型接着训练\n",
    "model.compile(optimizer = 'adam',\n",
    "#               optimizer=SGD(lr=0.001, momentum=0.9,decay=0.0001,nesterov=True),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    " \n",
    "\n",
    "\n",
    "#更好地保存模型 Save the model after every epoch.\n",
    "output_model_file = 'densenet_checkpoint-{epoch:02d}e-val_acc_{val_acc:.2f}.hdf5'\n",
    "# keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "checkpoint = ModelCheckpoint(output_model_file, monitor='val_acc',\n",
    "                             verbose=1, save_best_only=True)\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\n",
    "                                                patience=4,\n",
    "                                                verbose=2,\n",
    "                                                factor=.3,\n",
    "                                                min_lr=.00001)\n",
    " \n",
    "#开始训练\n",
    "history_ft = model.fit_generator(\n",
    "                                train_generator,\n",
    "                                nb_epoch=30,\n",
    "                                validation_data=valid_generator,\n",
    "                                validation_steps=val_shape // batch_size,\n",
    "#                                 verbose=2,\n",
    "                                steps_per_epoch=train_shape // batch_size,\n",
    "                                callbacks=[learning_rate_reduction,checkpoint],\n",
    ")\n",
    "'''\n",
    "def plot_training(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(len(acc))\n",
    "    plt.plot(epochs, acc, 'r-')\n",
    "    plt.plot(epochs, val_acc, 'b')\n",
    "    plt.legend(['training','val'])\n",
    "    plt.title('Training and validation accuracy')\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'r-')\n",
    "    plt.plot(epochs, val_loss, 'b-')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend(['training','val'])\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# 训练的acc_loss图\n",
    "# plot_training(history_ft)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Xception Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.xception import Xception,preprocess_input\n",
    "from keras.layers import Input,Dense,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnb_classes = 3 # 分类数\\nnb_epoch = int(30)                # epoch数量\\nbatch_size = int(32)        \\ntarget_size = (128,173)\\ntrain_shape = 3600  # 训练样本个数\\nval_shape = 900  # 验证样本个数\\n\\n\\nbase_model = Xception(include_top=False ) #input_tensor=input_image,  \\nx = base_model.output\\nx = GlobalAveragePooling2D()(x)\\n# x = Dropout(0.5)(x)\\n# x = Dense(1024, activation='relu')(x)\\nx = Dropout(0.5)(x)\\npredicts =Dense(3, activation='softmax')(x) # for i in range(3) ] \\n\\n                                          \\nxception_model = Model(inputs=base_model.input, outputs=predicts)  #\\nxception_model.compile(optimizer='adam',\\n                       loss='categorical_crossentropy', metrics=['accuracy'])\\n\\n\\nx_output_model_file = 'x_checkpoint-{epoch:02d}e-val_acc_{val_acc:.2f}.hdf5'\\n\\nx_checkpoint = ModelCheckpoint(x_output_model_file, monitor='val_acc',\\n                             verbose=1, save_best_only=True)\\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\\n                                                patience=4,\\n                                                verbose=2,\\n                                                factor=.3,\\n                                                min_lr=.00001)\\n\\nxception_history_ft = xception_model.fit_generator(\\n                                train_generator,\\n                                nb_epoch=nb_epoch,  #\\n                                validation_data=valid_generator,\\n                                validation_steps=val_shape // batch_size,\\n#                                 verbose=2,\\n                                steps_per_epoch=train_shape // batch_size,\\n                                callbacks=[learning_rate_reduction,x_checkpoint], #\\n)\\n \\n    \\n# 训练的acc_loss图\\nplot_training(xception_history_ft)\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "nb_classes = 3 # 分类数\n",
    "nb_epoch = int(30)                # epoch数量\n",
    "batch_size = int(32)        \n",
    "target_size = (128,173)\n",
    "train_shape = 3600  # 训练样本个数\n",
    "val_shape = 900  # 验证样本个数\n",
    "\n",
    "\n",
    "base_model = Xception(include_top=False ) #input_tensor=input_image,  \n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predicts =Dense(3, activation='softmax')(x) # for i in range(3) ] \n",
    "\n",
    "                                          \n",
    "xception_model = Model(inputs=base_model.input, outputs=predicts)  #\n",
    "xception_model.compile(optimizer='adam',\n",
    "                       loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "x_output_model_file = 'x_checkpoint-{epoch:02d}e-val_acc_{val_acc:.2f}.hdf5'\n",
    "\n",
    "x_checkpoint = ModelCheckpoint(x_output_model_file, monitor='val_acc',\n",
    "                             verbose=1, save_best_only=True)\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\n",
    "                                                patience=4,\n",
    "                                                verbose=2,\n",
    "                                                factor=.3,\n",
    "                                                min_lr=.00001)\n",
    "\n",
    "xception_history_ft = xception_model.fit_generator(\n",
    "                                train_generator,\n",
    "                                nb_epoch=nb_epoch,  #\n",
    "                                validation_data=valid_generator,\n",
    "                                validation_steps=val_shape // batch_size,\n",
    "#                                 verbose=2,\n",
    "                                steps_per_epoch=train_shape // batch_size,\n",
    "                                callbacks=[learning_rate_reduction,x_checkpoint], #\n",
    ")\n",
    " \n",
    "    \n",
    "# 训练的acc_loss图\n",
    "plot_training(xception_history_ft)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) multi model测试\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 存储 Densenet Xception 模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__output__.json', '__notebook__.ipynb']\n",
      "\n",
      "[]\n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# 存储 \n",
    "\n",
    "import os\n",
    "files = os.listdir(\"../working\")\n",
    "print(files)\n",
    "x_model_checkpoint = []\n",
    "dense_model_checkpoint = []\n",
    "for file in files:\n",
    "    if file.startswith('densenet_checkpoint'):  # densenet checkpoint\n",
    "        dense_model_checkpoint.append(file)\n",
    "    if file.startswith('x_checkpoint'):  # xception checkpoint\n",
    "        x_model_checkpoint.append(file)\n",
    "        \n",
    "x_model_checkpoint = sorted(x_model_checkpoint)\n",
    "dense_model_checkpoint = sorted(dense_model_checkpoint)\n",
    "print()\n",
    "print(dense_model_checkpoint)\n",
    "print()\n",
    "print(x_model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载 Densenet Xception 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xception=Xception(include_top=False,weights=None,) #input_shape=(224,224,3)\n",
    "# dense_model = load_model('densenet_checkpoint-30e-val_acc_0.77.hdf5')\n",
    "# # dense_model.save_weights('densenet_30e_0.77.h5')\n",
    "# xception_model = load_model('x_checkpoint-22e-val_acc_0.82.hdf5')\n",
    "# xception_model.save_weights('xception_22e_0.82.h5')\n",
    "\n",
    "## 直接输出的话disk爆掉\n",
    "# dense_model = load_model(dense_model_checkpoint[-1])\n",
    "# xception_model = load_model(x_model_checkpoint[-1])\n",
    "\n",
    "dense_model = load_model('../input/multimodel/densenet_checkpoint-26e-val_acc_0.80.hdf5')\n",
    "xception_model = load_model('../input/multimodel2/x_checkpoint-26e-val_acc_0.80.hdf5')\n",
    "# print(x_model_checkpoint[-1])\n",
    "# xception_model = load_model(x_model_checkpoint[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__output__.json', '__notebook__.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir('../working'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenseNet Xception 模型融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Multimodel(dense_model,xception_model,all_weights_path=None,class_num=3,cnn_no_vary=False):\n",
    "    '''\n",
    "    获取densent201,xinception并联的网络\n",
    "    此处的cnn_weights_path是个列表是densenet和xception的卷积部分的权值\n",
    "    '''\n",
    "    input_layer=Input(shape=(128,173,3))\n",
    "    dense=DenseNet201(include_top=False,weights=None) #,input_shape=(224,224,3)\n",
    "    xception=Xception(include_top=False,weights=None,) #input_shape=(224,224,3)\n",
    "    \n",
    "    xception.load_weights('../input/multimodel2/x_checkpoint-26e-val_acc_0.80.hdf5', by_name = True)\n",
    "    dense.load_weights('../input/multimodel/densenet_checkpoint-26e-val_acc_0.80.hdf5', by_name = True)\n",
    " \n",
    " \n",
    "    if cnn_no_vary:\n",
    "        for i,layer in  enumerate(dense.layers):\n",
    "            dense.layers[i].trainable=False\n",
    "        for i,layer in enumerate(xception.layers):\n",
    "            xception.layers[i].trainable=False\n",
    "       \n",
    " \n",
    "   # if cnn_weights_path!=None:\n",
    "#         dense.load_weights(cnn_weights_path[0])\n",
    "#         xception.load_weights(cnn_weights_path[1])\n",
    "##     dense = dense_model\n",
    "##     xception = xception_model\n",
    "#     dense_model.name = 'model_dense'   # m\n",
    "#     xception_model.name = 'model_xception'\n",
    "    \n",
    "#     dense=dense_model(input_layer)\n",
    "#     xception=xception_model(input_layer)\n",
    "    dense=dense (input_layer)\n",
    "    xception=xception (input_layer)\n",
    "    #对dense_121和xception进行全局最大池化\n",
    "    top1_model=GlobalMaxPooling2D()(dense)# dense #data_format='channels_last'GlobalMaxPooling2D\n",
    "    top2_model=GlobalMaxPooling2D()(xception)#xception#data_format='channels_last'GlobalMaxPooling2D\n",
    "\n",
    "\n",
    "    print(top1_model.shape,top2_model.shape)\n",
    "    #把top1_model和top2_model连接起来\n",
    "    t=keras.layers.Concatenate(axis=1)([top1_model,top2_model])\n",
    "    #第一个全连接层\n",
    "    top_model=Dense(units=128,activation=\"relu\")(t)\n",
    "    top_model=Dropout(rate=0.5)(top_model)\n",
    "#     top_model=Dense(units=256,activation=\"relu\")(t)\n",
    "#     top_model=Dropout(rate=0.7)(top_model)\n",
    "    top_model=Dense(units=class_num,activation=\"softmax\")(top_model)\n",
    "\n",
    "#     \n",
    "    model=Model(inputs=input_layer,outputs=top_model)\n",
    "\n",
    " \n",
    "    #加载全部的参数\n",
    "    if all_weights_path:\n",
    "        model.load_weights(all_weights_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1920) (?, 2048)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "multi_model=Multimodel(dense_model,xception_model,class_num=3)\n",
    "# multi_model = load_model('../input/multimodel3/multi_checkpoint-23e-val_acc_0.82.hdf5',compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:55: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:55: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=<keras_pre..., validation_steps=28, steps_per_epoch=112, callbacks=[<keras.ca..., epochs=30)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/opt/conda/lib/python3.6/site-packages/keras_preprocessing/image/image_data_generator.py:735: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 154s 1s/step - loss: 0.5206 - acc: 0.9121 - val_loss: 3.5892 - val_acc: 0.6440\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.64397, saving model to multi_checkpoint-01e-val_acc_0.64.hdf5\n",
      "Epoch 2/30\n",
      "112/112 [==============================] - 63s 562ms/step - loss: 0.1481 - acc: 0.9587 - val_loss: 1.8858 - val_acc: 0.7373\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.64397 to 0.73733, saving model to multi_checkpoint-02e-val_acc_0.74.hdf5\n",
      "Epoch 3/30\n",
      "112/112 [==============================] - 61s 549ms/step - loss: 0.1052 - acc: 0.9660 - val_loss: 2.5077 - val_acc: 0.6601\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.73733\n",
      "Epoch 4/30\n",
      "112/112 [==============================] - 61s 545ms/step - loss: 0.0884 - acc: 0.9752 - val_loss: 1.8526 - val_acc: 0.7442\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.73733 to 0.74424, saving model to multi_checkpoint-04e-val_acc_0.74.hdf5\n",
      "Epoch 5/30\n",
      "112/112 [==============================] - 62s 551ms/step - loss: 0.0781 - acc: 0.9752 - val_loss: 2.0028 - val_acc: 0.7120\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.74424\n",
      "Epoch 6/30\n",
      "112/112 [==============================] - 61s 545ms/step - loss: 0.1660 - acc: 0.9534 - val_loss: 1.4177 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.74424\n",
      "Epoch 7/30\n",
      "112/112 [==============================] - 61s 546ms/step - loss: 0.1373 - acc: 0.9584 - val_loss: 2.7816 - val_acc: 0.6452\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.74424\n",
      "Epoch 8/30\n",
      "112/112 [==============================] - 61s 547ms/step - loss: 0.1493 - acc: 0.9565 - val_loss: 1.2262 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.74424\n",
      "Epoch 9/30\n",
      "112/112 [==============================] - 62s 552ms/step - loss: 0.0608 - acc: 0.9796 - val_loss: 1.1425 - val_acc: 0.7615\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.74424 to 0.76152, saving model to multi_checkpoint-09e-val_acc_0.76.hdf5\n",
      "Epoch 10/30\n",
      "112/112 [==============================] - 62s 552ms/step - loss: 0.0133 - acc: 0.9961 - val_loss: 1.4094 - val_acc: 0.7684\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.76152 to 0.76843, saving model to multi_checkpoint-10e-val_acc_0.77.hdf5\n",
      "Epoch 11/30\n",
      "112/112 [==============================] - 62s 552ms/step - loss: 0.0075 - acc: 0.9978 - val_loss: 1.5458 - val_acc: 0.7765\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.76843 to 0.77650, saving model to multi_checkpoint-11e-val_acc_0.78.hdf5\n",
      "Epoch 12/30\n",
      "112/112 [==============================] - 62s 552ms/step - loss: 0.0046 - acc: 0.9980 - val_loss: 1.6802 - val_acc: 0.7903\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.77650 to 0.79032, saving model to multi_checkpoint-12e-val_acc_0.79.hdf5\n",
      "Epoch 13/30\n",
      "112/112 [==============================] - 62s 553ms/step - loss: 0.0024 - acc: 0.9989 - val_loss: 1.7387 - val_acc: 0.7776\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.79032\n",
      "Epoch 14/30\n",
      "112/112 [==============================] - 62s 553ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 1.8453 - val_acc: 0.7892\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.79032\n",
      "Epoch 15/30\n",
      "112/112 [==============================] - 61s 548ms/step - loss: 0.0024 - acc: 0.9994 - val_loss: 1.6426 - val_acc: 0.7961\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.79032 to 0.79608, saving model to multi_checkpoint-15e-val_acc_0.80.hdf5\n",
      "Epoch 16/30\n",
      "112/112 [==============================] - 62s 554ms/step - loss: 0.0063 - acc: 0.9992 - val_loss: 2.0353 - val_acc: 0.7846\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.79608\n",
      "Epoch 17/30\n",
      "112/112 [==============================] - 61s 548ms/step - loss: 0.0010 - acc: 0.9997 - val_loss: 1.9604 - val_acc: 0.7788\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.79608\n",
      "Epoch 18/30\n",
      "112/112 [==============================] - 61s 546ms/step - loss: 0.0025 - acc: 0.9994 - val_loss: 2.1002 - val_acc: 0.7742\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.79608\n",
      "Epoch 19/30\n",
      "112/112 [==============================] - 62s 550ms/step - loss: 0.0412 - acc: 0.9900 - val_loss: 1.5725 - val_acc: 0.7869\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.79608\n",
      "Epoch 20/30\n",
      "112/112 [==============================] - 61s 543ms/step - loss: 0.0068 - acc: 0.9983 - val_loss: 1.4765 - val_acc: 0.7938\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.79608\n",
      "Epoch 21/30\n",
      "112/112 [==============================] - 61s 544ms/step - loss: 0.0103 - acc: 0.9969 - val_loss: 1.7680 - val_acc: 0.7800\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.79608\n",
      "Epoch 22/30\n",
      "112/112 [==============================] - 61s 544ms/step - loss: 0.0029 - acc: 0.9992 - val_loss: 1.6645 - val_acc: 0.7776\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.79608\n",
      "Epoch 23/30\n",
      "112/112 [==============================] - 61s 543ms/step - loss: 0.0033 - acc: 0.9989 - val_loss: 1.5996 - val_acc: 0.7938\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.79608\n",
      "Epoch 24/30\n",
      "112/112 [==============================] - 62s 550ms/step - loss: 0.0026 - acc: 0.9994 - val_loss: 1.6934 - val_acc: 0.7903\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.79608\n",
      "Epoch 25/30\n",
      "112/112 [==============================] - 60s 538ms/step - loss: 0.0020 - acc: 0.9997 - val_loss: 1.6166 - val_acc: 0.7915\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.79608\n",
      "Epoch 26/30\n",
      "112/112 [==============================] - 60s 539ms/step - loss: 0.0019 - acc: 0.9994 - val_loss: 1.5417 - val_acc: 0.8053\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.79608 to 0.80530, saving model to multi_checkpoint-26e-val_acc_0.81.hdf5\n",
      "Epoch 27/30\n",
      "112/112 [==============================] - 61s 544ms/step - loss: 0.0032 - acc: 0.9994 - val_loss: 1.7841 - val_acc: 0.7926\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.80530\n",
      "Epoch 28/30\n",
      "112/112 [==============================] - 60s 539ms/step - loss: 0.0019 - acc: 0.9994 - val_loss: 1.6338 - val_acc: 0.8018\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.80530\n",
      "Epoch 29/30\n",
      "112/112 [==============================] - 61s 545ms/step - loss: 0.0031 - acc: 0.9986 - val_loss: 1.6884 - val_acc: 0.7949\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.80530\n",
      "Epoch 30/30\n",
      "112/112 [==============================] - 61s 541ms/step - loss: 5.5022e-04 - acc: 1.0000 - val_loss: 1.6962 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.80530\n"
     ]
    }
   ],
   "source": [
    "# for layer in multi_model.layers:\n",
    "#     layer.trainable = False#原来的不训练\n",
    "        \n",
    "# inp=multi_model.input\n",
    "# x=multi_model.output\n",
    "# x = Input(( 128, 173, 3))\n",
    "# # l=Conv2D(filters=32, kernel_size=(3, 3), padding='Same',\n",
    "# #                      activation='relu', input_shape=(None,128, 173, 3))(x)\n",
    "# # # Conv2D(64, 3, 3, border_mode='same',init='glorot_uniform')(reshape)\n",
    "# # # l=PReLU()(l)\n",
    "# # l=BatchNormalization()(l)\n",
    "\n",
    "# l=GlobalAveragePooling2D()(x)\n",
    "# den=Dense(512,name=\"fine_dense4\")(x)\n",
    "# l=PReLU()(den)\n",
    "# # l=GlobalAveragePooling2D()(l)\n",
    "# l=Dropout(0.5)(l)\n",
    "# # result=Dense(3,activation=\"softmax\")(l)\n",
    "# den=Dense(1024,name=\"fine_dense6\")(x)\n",
    "# l=PReLU()(den)\n",
    "\n",
    "# l=Dropout(0.7)(l)\n",
    "# # l=GlobalAveragePooling2D()(l)\n",
    "# result=Dense(3,activation=\"softmax\")(l)\n",
    "\n",
    "\n",
    "# multi_model=Model(input=inp,outputs=result)\n",
    "\n",
    "#编译model\n",
    "\n",
    "\n",
    "\n",
    "# 模型编译\n",
    "multi_model.compile(optimizer='adam',\n",
    "                    loss='categorical_crossentropy', \n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "multi_output_model_file = 'multi_checkpoint-{epoch:02d}e-val_acc_{val_acc:.2f}.hdf5'\n",
    "\n",
    "multi_checkpoint = ModelCheckpoint(multi_output_model_file, monitor='val_acc',\n",
    "                             verbose=1, save_best_only=True)\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\n",
    "                                                patience=4,\n",
    "                                                verbose=2,\n",
    "                                                factor=.3,\n",
    "                                                min_lr=.00001)\n",
    "\n",
    "multi_history_ft = multi_model.fit_generator(\n",
    "                                train_generator,\n",
    "                                nb_epoch=30,  #\n",
    "                                validation_data=valid_generator,\n",
    "                                validation_steps=val_shape // batch_size,\n",
    "\n",
    "                                steps_per_epoch=train_shape // batch_size,\n",
    "                                callbacks=[learning_rate_reduction,multi_checkpoint], #\n",
    ")\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 存储融合模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['multi_checkpoint-15e-val_acc_0.80.hdf5', 'multi_checkpoint-02e-val_acc_0.74.hdf5', 'multi_checkpoint-04e-val_acc_0.74.hdf5', 'multi_checkpoint-11e-val_acc_0.78.hdf5', 'multi_checkpoint-12e-val_acc_0.79.hdf5', 'multi_checkpoint-26e-val_acc_0.81.hdf5', 'multi_checkpoint-01e-val_acc_0.64.hdf5', '__output__.json', 'multi_checkpoint-10e-val_acc_0.77.hdf5', '__notebook__.ipynb', 'multi_checkpoint-09e-val_acc_0.76.hdf5']\n",
      "\n",
      "['multi_checkpoint-01e-val_acc_0.64.hdf5', 'multi_checkpoint-02e-val_acc_0.74.hdf5', 'multi_checkpoint-04e-val_acc_0.74.hdf5', 'multi_checkpoint-09e-val_acc_0.76.hdf5', 'multi_checkpoint-10e-val_acc_0.77.hdf5', 'multi_checkpoint-11e-val_acc_0.78.hdf5', 'multi_checkpoint-12e-val_acc_0.79.hdf5', 'multi_checkpoint-15e-val_acc_0.80.hdf5', 'multi_checkpoint-26e-val_acc_0.81.hdf5']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files = os.listdir(\"../working\")\n",
    "print(files)\n",
    "print()\n",
    "multi_checkpoint = []\n",
    "for file in files:\n",
    "\n",
    "    if file.startswith('multi_checkpoint'):  # multi checkpoint\n",
    "        multi_checkpoint.append(file)\n",
    "multi_checkpoint = sorted(multi_checkpoint)      \n",
    "print((multi_checkpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BottleNeck 特征提升准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n######### 1. 生成训练集与测试集的bottleneck特征\\n\\n# include_top：是否保留顶层的3个全连接网络\\n# 'imagenet'代表加载预训练权重\\nmodel = VGG16(include_top=False, weights='imagenet')\\n# 加载pre-model的权重\\nmodel.load_weights('vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5')\\n\\n\\n# 训练集图像生成器,以文件夹路径为参数,生成经过数据提升/归一化后的数据\\n\\n\\n# 得到bottleneck feature\\n# 使用一个生成器作为数据源预测模型，生成器应返回与test_on_batch的输入数据相同类型的数据\\nbottleneck_features_train = model.predict_generator(train_generator, steps=len(train_generator)) \\nprint (bottleneck_features_train) \\n# steps是生成器要返回数据的轮数\\n# 将得到的特征记录在numpy array里\\nnp.save(open('bottleneck_features_train.npy', 'w'), bottleneck_features_train)\\nbottleneck_features_validation = model.predict_generator(val_generator, steps=len(val_generator)) \\n# 一个epoch有800张图片,验证集\\nnp.save(open('bottleneck_features_validation.npy', 'w'), bottleneck_features_validation)\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "######### 1. 生成训练集与测试集的bottleneck特征\n",
    "\n",
    "# include_top：是否保留顶层的3个全连接网络\n",
    "# 'imagenet'代表加载预训练权重\n",
    "model = VGG16(include_top=False, weights='imagenet')\n",
    "# 加载pre-model的权重\n",
    "model.load_weights('vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "\n",
    "\n",
    "# 训练集图像生成器,以文件夹路径为参数,生成经过数据提升/归一化后的数据\n",
    "\n",
    "\n",
    "# 得到bottleneck feature\n",
    "# 使用一个生成器作为数据源预测模型，生成器应返回与test_on_batch的输入数据相同类型的数据\n",
    "bottleneck_features_train = model.predict_generator(train_generator, steps=len(train_generator)) \n",
    "print (bottleneck_features_train) \n",
    "# steps是生成器要返回数据的轮数\n",
    "# 将得到的特征记录在numpy array里\n",
    "np.save(open('bottleneck_features_train.npy', 'w'), bottleneck_features_train)\n",
    "bottleneck_features_validation = model.predict_generator(val_generator, steps=len(val_generator)) \n",
    "# 一个epoch有800张图片,验证集\n",
    "np.save(open('bottleneck_features_validation.npy', 'w'), bottleneck_features_validation)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### (3) InceptionResNetV2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.nasnet import NASNetLarge\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean acc  : - training: 0.7057491324232396           \n",
    "Mean acc  : - val: 0.5350085034013604         \n",
    "Mean loss : - training: 0.7226675915719312            \n",
    "Mean loss : - val: 2.2375190057559884'\n",
    "准确率并不高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xception Densenet 迁移学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('../input/speech-train2/index.csv')\n",
    "# train_df['accent'] = train_df['accent'].astype('str')\n",
    "# print(type(train_df['accent'].values))\n",
    "# train_df['file_id'] = \"../input/speech-train2/train/train/\" +train_df['file_id']\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#  训练集、测试集\\n# 数据准备\\nIM_WIDTH, IM_HEIGHT = 128, 173 #densenet指定的图片尺寸\\n\\nnb_classes = 3 # 分类数\\nbatch_size = int(32)        \\ntarget_size = (128,173)\\n\\n\\n\\n# 训练数据与测试数据\\ndatagen = image.ImageDataGenerator(preprocessing_function=None,   #preprocess_input\\n                             rescale=1./255.,\\n                            zca_whitening = True,    \\n                             validation_split=0.2)\\n \\ntrain_generator=datagen.flow_from_dataframe(\\n                        dataframe=train_df,\\n                        directory=None, \\n                        x_col=\"file_id\", \\n                        y_col=\"accent\",\\n                        has_ext=False,\\n                        batch_size=batch_size,\\n                        seed=42,\\n                        shuffle=True,\\n                        class_mode=\"categorical\",\\n                        target_size=target_size)  #(128, 173)\\n\\n\\nfolder_dir = \"../input/speech-test/test/test/\"  #测试集\\n\\ntest_df = pd.DataFrame(columns=[\\'file_id\\',\\'accent\\'])\\ni= 0 \\n\\nfor filename in os.listdir(folder_dir):\\n    file = folder_dir+filename\\n    test_df.at[i,\\'file_id\\'] = file\\n    i+=1\\n\\nprint(test_df.shape)\\n\\ntest_datagen=image.ImageDataGenerator(rescale=1./255.,\\n                                     preprocessing_function=None,   #preprocess_input\\n                             )\\n\\ntest_generator=test_datagen.flow_from_dataframe(\\n                            dataframe=test_df,\\n                            directory=None,\\n                            x_col=\"file_id\",\\n                            y_col=None,\\n                            has_ext=False,\\n                            batch_size=batch_size,#1\\n                            seed=42,\\n                            shuffle=False,\\n                            class_mode=None,\\n                            target_size=target_size)\\ntest_df.head()\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#  训练集、测试集\n",
    "# 数据准备\n",
    "IM_WIDTH, IM_HEIGHT = 128, 173 #densenet指定的图片尺寸\n",
    "\n",
    "nb_classes = 3 # 分类数\n",
    "batch_size = int(32)        \n",
    "target_size = (128,173)\n",
    "\n",
    "\n",
    "\n",
    "# 训练数据与测试数据\n",
    "datagen = image.ImageDataGenerator(preprocessing_function=None,   #preprocess_input\n",
    "                             rescale=1./255.,\n",
    "                            zca_whitening = True,    \n",
    "                             validation_split=0.2)\n",
    " \n",
    "train_generator=datagen.flow_from_dataframe(\n",
    "                        dataframe=train_df,\n",
    "                        directory=None, \n",
    "                        x_col=\"file_id\", \n",
    "                        y_col=\"accent\",\n",
    "                        has_ext=False,\n",
    "                        batch_size=batch_size,\n",
    "                        seed=42,\n",
    "                        shuffle=True,\n",
    "                        class_mode=\"categorical\",\n",
    "                        target_size=target_size)  #(128, 173)\n",
    "\n",
    "\n",
    "folder_dir = \"../input/speech-test/test/test/\"  #测试集\n",
    "\n",
    "test_df = pd.DataFrame(columns=['file_id','accent'])\n",
    "i= 0 \n",
    "\n",
    "for filename in os.listdir(folder_dir):\n",
    "    file = folder_dir+filename\n",
    "    test_df.at[i,'file_id'] = file\n",
    "    i+=1\n",
    "\n",
    "print(test_df.shape)\n",
    "\n",
    "test_datagen=image.ImageDataGenerator(rescale=1./255.,\n",
    "                                     preprocessing_function=None,   #preprocess_input\n",
    "                             )\n",
    "\n",
    "test_generator=test_datagen.flow_from_dataframe(\n",
    "                            dataframe=test_df,\n",
    "                            directory=None,\n",
    "                            x_col=\"file_id\",\n",
    "                            y_col=None,\n",
    "                            has_ext=False,\n",
    "                            batch_size=batch_size,#1\n",
    "                            seed=42,\n",
    "                            shuffle=False,\n",
    "                            class_mode=None,\n",
    "                            target_size=target_size)\n",
    "test_df.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## 1. 生成迁移学习特征变量\\ndef write_gap(MODEL, model_name,lambda_func=None):\\n    width = 173\\n    height = 128\\n    input_tensor = Input((height, width, 3))\\n    x = input_tensor\\n    if lambda_func:\\n        x = Lambda(lambda_func)(x)\\n    base_model = MODEL(input_tensor=x,  weights=\\'imagenet\\',include_top=False, )  #weights=\\'imagenet\\',pooling=\\'avg\\'\\n#     x = base_model.output\\n#     x = GlobalAveragePooling2D()(x)\\n#     x = Dropout(0.5)(x)\\n#     predicts =Dense(3, activation=\\'softmax\\')(x) # for i in range(3) ] \\n\\n#     model = Model(base_model.input, predicts)\\n    model = Model(base_model.input,GlobalAveragePooling2D()(base_model.output) )\\n    \\n    train = model.predict_generator(train_generator, len(train_generator))\\n    test = model.predict_generator(test_generator, len(test_generator))\\n\\n    with h5py.File(\"gap_%s.h5\"%model_name) as h:\\n        h.create_dataset(\"train\", data=train)\\n        h.create_dataset(\"test\", data=test)\\n        h.create_dataset(\"label\", data=train_generator.classes)\\n        '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "## 1. 生成迁移学习特征变量\n",
    "def write_gap(MODEL, model_name,lambda_func=None):\n",
    "    width = 173\n",
    "    height = 128\n",
    "    input_tensor = Input((height, width, 3))\n",
    "    x = input_tensor\n",
    "    if lambda_func:\n",
    "        x = Lambda(lambda_func)(x)\n",
    "    base_model = MODEL(input_tensor=x,  weights='imagenet',include_top=False, )  #weights='imagenet',pooling='avg'\n",
    "#     x = base_model.output\n",
    "#     x = GlobalAveragePooling2D()(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     predicts =Dense(3, activation='softmax')(x) # for i in range(3) ] \n",
    "\n",
    "#     model = Model(base_model.input, predicts)\n",
    "    model = Model(base_model.input,GlobalAveragePooling2D()(base_model.output) )\n",
    "    \n",
    "    train = model.predict_generator(train_generator, len(train_generator))\n",
    "    test = model.predict_generator(test_generator, len(test_generator))\n",
    "\n",
    "    with h5py.File(\"gap_%s.h5\"%model_name) as h:\n",
    "        h.create_dataset(\"train\", data=train)\n",
    "        h.create_dataset(\"test\", data=test)\n",
    "        h.create_dataset(\"label\", data=train_generator.classes)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwrite_gap(Xception,'Xception')\\nwrite_gap(DenseNet201,'DenseNet201')\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "write_gap(Xception,'Xception')\n",
    "write_gap(DenseNet201,'DenseNet201')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. 载入特征变量\n",
    "# np.random.seed(2019)\n",
    "\n",
    "# X_train = []\n",
    "# X_test = []\n",
    "\n",
    "# for filename in ['gap_DenseNet201.h5', 'gap_Xception.h5',]:\n",
    "#     with h5py.File(filename, 'r') as h:\n",
    "        \n",
    "#         X_train.append(np.array(h['train']))\n",
    "#         X_test.append(np.array(h['test']))\n",
    "#         y_train = np.array(h['label'])\n",
    "#         print(h['train'],h['test'])\n",
    "\n",
    "# X_train = np.concatenate(X_train, axis=1)\n",
    "# X_test = np.concatenate(X_test, axis=1)\n",
    "# y_train = keras.utils.to_categorical(y_train , num_classes=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. 构建模型\n",
    "# print(X_train.shape)\n",
    "# input_tensor = Input(X_train.shape[1:])\n",
    "# x = input_tensor\n",
    "# # x = Dropout(0.5)(x)\n",
    "# # x = Dense(1024, activation='relu')(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(3, activation='softmax')(x)\n",
    "# model = Model( input_tensor,x) #\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='categorical_crossentropy', #categorical\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. 训练模型\n",
    "\n",
    "\n",
    "# model_history = model.fit(X_train, \n",
    "#                     y_train,\n",
    "#                     batch_size=32,\n",
    "#                     epochs=20,\n",
    "#                     verbose=1,\n",
    "#                     validation_split=0.2,\n",
    "# #                     callbacks = [keras.callbacks.TensorBoard(log_dir='./Graph',)] #TensorBoard(log_dir='./Graph')\n",
    "# )\n",
    "# model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5377, 2)\n",
      "Found 5377 validated image filenames.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>accent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../input/speech-test/test/test/21352.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../input/speech-test/test/test/21121.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../input/speech-test/test/test/23559.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../input/speech-test/test/test/20365.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../input/speech-test/test/test/24472.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    file_id accent\n",
       "0  ../input/speech-test/test/test/21352.png    NaN\n",
       "1  ../input/speech-test/test/test/21121.png    NaN\n",
       "2  ../input/speech-test/test/test/23559.png    NaN\n",
       "3  ../input/speech-test/test/test/20365.png    NaN\n",
       "4  ../input/speech-test/test/test/24472.png    NaN"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试集\n",
    "\n",
    "folder_dir = \"../input/speech-test/test/test/\"  #测试集\n",
    "\n",
    "test_df = pd.DataFrame(columns=['file_id','accent'])\n",
    "i= 0 \n",
    "\n",
    "for filename in os.listdir(folder_dir):\n",
    "    file = folder_dir+filename\n",
    "    test_df.at[i,'file_id'] = file\n",
    "    i+=1\n",
    "\n",
    "print(test_df.shape)\n",
    "\n",
    "test_datagen=image.ImageDataGenerator(rescale=1./255.,\n",
    "                                     preprocessing_function=preprocess_input,   #None\n",
    "                             )\n",
    "\n",
    "test_generator=test_datagen.flow_from_dataframe(\n",
    "                            dataframe=test_df,\n",
    "                            directory=None,\n",
    "                            x_col=\"file_id\",\n",
    "                            y_col=None,\n",
    "                            has_ext=False,\n",
    "                            batch_size=batch_size,#1\n",
    "                            seed=42,\n",
    "                            shuffle=False,\n",
    "                            class_mode=None,\n",
    "                            target_size=target_size)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型预测测试集 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(model , test_generator, name):\n",
    "    # 模型预测\n",
    "    \n",
    "    pred = model.predict_generator(test_generator,steps= len(test_generator)) \n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(columns=['file_id','accent'])  #输出文件\n",
    "    i= 0\n",
    "\n",
    "    predicted_class_indices = np.argmax(pred, axis=1)\n",
    "\n",
    "    \n",
    "    # # #建立预测结果和文件名之间的关系\n",
    "    filenames = test_generator.filenames\n",
    "    for idx in range(len(filenames )):\n",
    "        df.at[i,'file_id'] = filenames[idx]\n",
    "        df.at[i,'accent'] = (int(predicted_class_indices[idx]))\n",
    "        i += 1\n",
    "#         print('predict  %d' % (int(predicted_class_indices[idx])),'title    %s' % filenames[idx])\n",
    "\n",
    "\n",
    "    df['file_id'] = df['file_id'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "    df = df.sort_values(by = 'file_id' , ascending = True)\n",
    "    print(df.head())\n",
    "    df.to_csv(name,index = False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmulti_model = load_model('../input/multimodel3/multi_checkpoint-23e-val_acc_0.82.hdf5',compile=False)\\n\\n\\n\\nmulti_model.compile(optimizer='adam',\\n                       loss='categorical_crossentropy', metrics=['accuracy'])\\n\\n\\n# x_output_model_file = 'loadmulti_checkpoint-{epoch:02d}e-val_acc_{val_acc:.2f}.hdf5'\\n\\n# x_checkpoint = ModelCheckpoint(x_output_model_file, monitor='val_acc',\\n#                              verbose=1, save_best_only=True)\\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\\n                                                patience=4,\\n                                                verbose=2,\\n                                                factor=.3,\\n                                                min_lr=.00001)\\n\\nmulti_history_ft = multi_model.fit_generator(\\n                                train_generator,\\n                                nb_epoch=30,  #\\n                                validation_data=valid_generator,\\n                                validation_steps=val_shape // batch_size,\\n                                steps_per_epoch=train_shape // batch_size,\\n                                callbacks=[learning_rate_reduction,], #x_checkpoint\\n)\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 加载已训练好的模型测试  模型过拟合？？？？？？？？？？？\n",
    "'''\n",
    "multi_model = load_model('../input/multimodel3/multi_checkpoint-23e-val_acc_0.82.hdf5',compile=False)\n",
    "\n",
    "\n",
    "\n",
    "multi_model.compile(optimizer='adam',\n",
    "                       loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# x_output_model_file = 'loadmulti_checkpoint-{epoch:02d}e-val_acc_{val_acc:.2f}.hdf5'\n",
    "\n",
    "# x_checkpoint = ModelCheckpoint(x_output_model_file, monitor='val_acc',\n",
    "#                              verbose=1, save_best_only=True)\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\n",
    "                                                patience=4,\n",
    "                                                verbose=2,\n",
    "                                                factor=.3,\n",
    "                                                min_lr=.00001)\n",
    "\n",
    "multi_history_ft = multi_model.fit_generator(\n",
    "                                train_generator,\n",
    "                                nb_epoch=30,  #\n",
    "                                validation_data=valid_generator,\n",
    "                                validation_steps=val_shape // batch_size,\n",
    "                                steps_per_epoch=train_shape // batch_size,\n",
    "                                callbacks=[learning_rate_reduction,], #x_checkpoint\n",
    ")\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['multi_checkpoint-15e-val_acc_0.80.hdf5', 'multi_checkpoint-02e-val_acc_0.74.hdf5', 'multi_checkpoint-04e-val_acc_0.74.hdf5', 'multi_checkpoint-11e-val_acc_0.78.hdf5', 'multi_checkpoint-12e-val_acc_0.79.hdf5', 'multi_checkpoint-26e-val_acc_0.81.hdf5', 'multi_checkpoint-01e-val_acc_0.64.hdf5', '__output__.json', 'multi_checkpoint-10e-val_acc_0.77.hdf5', '__notebook__.ipynb', 'multi_checkpoint-09e-val_acc_0.76.hdf5']\n",
      "\n",
      "['multi_checkpoint-01e-val_acc_0.64.hdf5', 'multi_checkpoint-02e-val_acc_0.74.hdf5', 'multi_checkpoint-04e-val_acc_0.74.hdf5', 'multi_checkpoint-09e-val_acc_0.76.hdf5', 'multi_checkpoint-10e-val_acc_0.77.hdf5', 'multi_checkpoint-11e-val_acc_0.78.hdf5', 'multi_checkpoint-12e-val_acc_0.79.hdf5', 'multi_checkpoint-15e-val_acc_0.80.hdf5', 'multi_checkpoint-26e-val_acc_0.81.hdf5']\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(\"../working\")\n",
    "print(files)\n",
    "print()\n",
    "loadmulti_checkpoint = []\n",
    "for file in files:\n",
    "\n",
    "    if file.startswith('loadmulti_checkpoint'):  # multi checkpoint\n",
    "        loadmulti_checkpoint.append(file)\n",
    "loadmulti_checkpoint = sorted(multi_checkpoint)      \n",
    "print((loadmulti_checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nloadmulti_model = load_model(loadmulti_checkpoint[-1],compile=False)\\ndf = output(loadmulti_model ,test_generator,'load_multimodel_submission.csv')\\nprint(df.accent.value_counts())\\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "loadmulti_model = load_model(loadmulti_checkpoint[-1],compile=False)\n",
    "df = output(loadmulti_model ,test_generator,'load_multimodel_submission.csv')\n",
    "print(df.accent.value_counts())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     file_id accent\n",
      "1753   20000      1\n",
      "2831   20001      1\n",
      "2603   20002      1\n",
      "1355   20003      1\n",
      "3850   20004      1\n",
      "1    5377\n",
      "Name: accent, dtype: int64\n",
      "     file_id accent\n",
      "1753   20000      2\n",
      "2831   20001      0\n",
      "2603   20002      2\n",
      "1355   20003      0\n",
      "3850   20004      1\n",
      "2    1998\n",
      "0    1905\n",
      "1    1474\n",
      "Name: accent, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# densenet预测输出\n",
    "# dense_model = load_model(dense_model_checkpoint[-1]) #选择最高的一个模型  #checkpoint[-1]\n",
    "# df = output(dense_model ,test_generator,'densenet_submission.csv')\n",
    "# print(df.accent.value_counts())\n",
    "\n",
    "# xception模型预测输出\n",
    "# xception_model = load_model(x_model_checkpoint[-1]) \n",
    "df = output(xception_model ,test_generator,'xception_submission.csv')  # 利用已加载的\n",
    "print(df.accent.value_counts())\n",
    "\n",
    "# 融合模型预测输出\n",
    "multi_model = load_model(multi_checkpoint[-1],compile=False) \n",
    "df = output( multi_model,test_generator,'multi_submission.csv')\n",
    "print(df.accent.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('densenet_submission.csv',index = False)\n",
    "# df.to_csv('xception_submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已建好模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.image as mpimg\n",
    "# from PIL import Image\n",
    "# import tensorflow as tf\n",
    "# from keras.preprocessing.image import img_to_array,load_img\n",
    "# from keras.preprocessing import image\n",
    "# from keras.applications.resnet50 import preprocess_input\n",
    "# import keras\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# print(keras.__version__)\n",
    "\n",
    "# model = tf.keras.models.load_model('../input/model3-30epoch0398/speech_6_2.h5' , compile = False)# 3个类别0.40\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_dir = \"../input/speech-test/test/test/\"  #测试集\n",
    "\n",
    "# test_df = pd.DataFrame(columns=['file_id','accent'])\n",
    "# i= 0 \n",
    "\n",
    "# for filename in os.listdir(folder_dir):\n",
    "#     file = folder_dir+filename\n",
    "#     test_df.at[i,'file_id'] = file\n",
    "#     i+=1\n",
    "\n",
    "# print(test_df.shape)\n",
    "\n",
    "# test_datagen=image.ImageDataGenerator(rescale=1./255.,\n",
    "#                                      preprocessing_function=None,   #preprocess_input\n",
    "                                   \n",
    "#                              )\n",
    "\n",
    "# test_generator=test_datagen.flow_from_dataframe(\n",
    "#                             dataframe=test_df,\n",
    "#                             directory=None,\n",
    "#                             x_col=\"file_id\",\n",
    "#                             y_col=None,\n",
    "#                             has_ext=False,\n",
    "#                             batch_size=32,#1\n",
    "#                             seed=42,\n",
    "#                             shuffle=False,\n",
    "#                             class_mode=None,\n",
    "#                             target_size=(128,173))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # pred = model.predict_generator(test_generator,steps= len(test_generator))\n",
    "# df = pd.DataFrame(columns=['file_id','accent'])  #输出文件\n",
    "# i= 0\n",
    "\n",
    "# predicted_class_indices = np.argmax(pred, axis=1)\n",
    "\n",
    "# # # #建立预测结果和文件名之间的关系\n",
    "# filenames = test_generator.filenames\n",
    "# for idx in range(len(filenames )):\n",
    "#     df.at[i,'file_id'] = filenames[idx]\n",
    "#     df.at[i,'accent'] = (int(predictions[idx]))\n",
    "#     i += 1\n",
    "#     print('predict  %d' % (int(predictions[idx])),'title    %s' % filenames[idx])\n",
    "\n",
    "    \n",
    "# df['file_id'] = df['file_id'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "# df = df.sort_values(by = 'file_id' , ascending = True)\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.accent.value_counts()\n",
    "# df.to_csv('loadmodel_submission.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
